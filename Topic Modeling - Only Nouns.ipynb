{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook creates a topic model using only nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sample3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:4401: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "#only keep nouns\n",
    "\n",
    "db = data[(data[\"pos\"]=='NN') | (data[\"pos\"]=='NNS') | (data[\"pos\"]=='NNP')| (data[\"pos\"]=='NNPS')]\n",
    "\n",
    "\n",
    "#retaining labels\n",
    "db_mapping = db[['chap_num','gender','id']].copy()\n",
    "db_mapping.drop_duplicates(inplace=True)\n",
    "print(len(db_mapping))\n",
    "db_mapping.head()\n",
    "\n",
    "#Aggregating corpus\n",
    "db.token_str = db.token_str.astype('str')\n",
    "cdb = db.groupby('chap_num')\\\n",
    "    .apply(lambda x: ' '.join(x.token_str))\\\n",
    "    .to_frame()\\\n",
    "    .rename(columns={0:'text'})\n",
    "cdb.head()\n",
    "\n",
    "#Merginig db\n",
    "dbm = pd.merge(cdb,db_mapping,left_index=True,right_on='chap_num')\n",
    "\n",
    "dbm.text = dbm.text.str.lower()\n",
    "dbm.text = dbm.text.str.replace('[^a-zA-Z]',\" \")\n",
    "dbm.text = dbm.text.str.replace('urllink',\" \")\n",
    "dbm.text = dbm.text.str.replace('nbsp',\" \")\n",
    "dbm.text = dbm.text.str.replace(r'\\n',' ')\n",
    "dbm.text = dbm.text.str.replace(r'\\s+',' ')\n",
    "dbm.text = dbm.text.str.replace('([ ]{2,})',' ')\n",
    "dbm = dbm[~dbm.text.str.match(r'^\\s*$')]\n",
    "dbm = dbm[dbm.text.apply(lambda x:len(x))>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = dbm.groupby('id')\\\n",
    ".apply(lambda x: ' '.join(x.text))\\\n",
    "    .to_frame()\\\n",
    "    .rename(columns={0:'text1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "cols = ['id', 'gender']\n",
    "\n",
    "gendid = dbm[cols]\n",
    "gendid.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = joined.merge(gendid, on='id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create tfidf and topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(min_df = .005,lowercase=True,stop_words='english',norm='l1',ngram_range=(1,2),max_features=1500).fit(joined['text1'])\n",
    "tfidf = vect.transform(joined['text1'])\n",
    "tfidf = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=0.35,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n",
       "             n_components=50, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=50, max_iter=50,learning_method='online', random_state=0,doc_topic_prior = .35)\n",
    "lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topic #{topic_idx}: \"\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: look peace break bar huh ben let winter sites socks\n",
      "Topic #1: person word end problem monday nights stop students york try\n",
      "Topic #2: world friends friend business feeling care information pool field form\n",
      "Topic #3: post yeah cool john bye experience yea dreams wind view\n",
      "Topic #4: lol kids page shot wan links son collection answer joy\n",
      "Topic #5: job team group water dinner order film beach cute alex\n",
      "Topic #6: guy computer thanks gon company reality went male brain david\n",
      "Topic #7: day pictures songs june plan just hehehe remember em television\n",
      "Topic #8: heart eyes fact rest pain games truth question children floor\n",
      "Topic #9: time week mind mouth ca market update butt heh tape\n",
      "Topic #10: food boy feelings books movies august middle moments matt boyfriend\n",
      "Topic #11: moment brother hair sorry new article crap road gmail service\n",
      "Topic #12: im http case note list video star sucks pages left\n",
      "Topic #13: weeks thats sort hmmm looks le corner window amp dogs\n",
      "Topic #14: morning couple city bed link problems sun energy pieces breath\n",
      "Topic #15: days life band wednesday katie jesus boys aunt lines tho\n",
      "Topic #16: hell shit share yes use weather gym eat bird spot\n",
      "Topic #17: thing thoughts month anyways test power action rain tears memory\n",
      "Topic #18: way music mother website number thought hands got space season\n",
      "Topic #19: haha sister coffee date ideas history camera today day meeting wedding\n",
      "Topic #20: work girls yay help bus sleep bitch pm area juz\n",
      "Topic #21: life course party fuck internet country posts message street training\n",
      "Topic #22: lot hours book phone store sense hehe clothes photo deal\n",
      "Topic #23: today home mom reason till project everybody daughter attempt harry potter\n",
      "Topic #24: weekend guys summer guess wow saturday office talk did concert\n",
      "Topic #25: things man ass ah july rock control touch card drop\n",
      "Topic #26: room dream tv blogger relationship women email thursday folks wife\n",
      "Topic #27: tonight web dont type hello paper camp ha play minute\n",
      "Topic #28: hand sunday smile men apartment sky hole chocolate result cos\n",
      "Topic #29: school house movie entry ppl afternoon child yahoo bob base\n",
      "Topic #30: game class birthday oh lunch http www shopping kitchen tuesday somebody\n",
      "Topic #31: hey place day ones living cousin age blood mike record\n",
      "Topic #32: girl half names evening stories bag reading think pics size\n",
      "Topic #33: yesterday body dark blogs teacher process level conversation stars mark\n",
      "Topic #34: blog years lots ride classes letters heck sarah awhile advice\n",
      "Topic #35: ok okay ya dad future didnt ball michael center photos\n",
      "Topic #36: hour war cuz online college google law walk miles mi\n",
      "Topic #37: com kind air period quiz choice parts pic homework info\n",
      "Topic #38: idea news words face www church title click dog mr\n",
      "Topic #39: year bit baby father iraq journal government act liao day\n",
      "Topic #40: right hope mood bush den attention blah kid cat wat\n",
      "Topic #41: fun times damn months wait line art mail box chris\n",
      "Topic #42: people site comments comment cd death situation america earth feel\n",
      "Topic #43: stuff minutes cause trip start track fault goal arms opinion\n",
      "Topic #44: god money picture friday parents alot haiz points soul ice\n",
      "Topic #45: song head la color voice blue drive memories da example\n",
      "Topic #46: tomorrow family kinda feet time time hi eye table fingers die\n",
      "Topic #47: night car matter lord change questions practice marriage event tree\n",
      "Topic #48: love town chance thinking door coz bunch kerry lives st\n",
      "Topic #49: point story thank woman sex hmm difference places writing player\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect topics\n",
    "\n",
    "tf_feature_names = vect.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find max topic for each document\n",
    "tr1['max'] = tr1.idxmax(axis = 1)\n",
    "finalDf = pd.concat([tr1, joined[['gender']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into different dfs for each gender\n",
    "male = finalDf[finalDf['gender'] == 'male']\n",
    "\n",
    "female = finalDf[finalDf['gender'] == 'female']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42    67\n",
       "5     67\n",
       "3     63\n",
       "11    61\n",
       "40    58\n",
       "24    57\n",
       "38    55\n",
       "49    55\n",
       "30    52\n",
       "44    52\n",
       "39    49\n",
       "2     47\n",
       "29    47\n",
       "26    44\n",
       "41    43\n",
       "6     42\n",
       "45    42\n",
       "43    40\n",
       "21    40\n",
       "12    39\n",
       "25    36\n",
       "46    34\n",
       "7     33\n",
       "48    33\n",
       "17    32\n",
       "18    32\n",
       "20    32\n",
       "0     32\n",
       "14    31\n",
       "15    30\n",
       "37    29\n",
       "8     28\n",
       "32    28\n",
       "22    28\n",
       "19    28\n",
       "4     27\n",
       "1     27\n",
       "23    26\n",
       "47    26\n",
       "27    26\n",
       "35    25\n",
       "9     23\n",
       "28    22\n",
       "34    21\n",
       "31    20\n",
       "36    20\n",
       "10    19\n",
       "33    18\n",
       "16    14\n",
       "13    14\n",
       "Name: max, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#male topic assignments\n",
    "\n",
    "male['max'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24    80\n",
       "3     63\n",
       "2     58\n",
       "26    57\n",
       "44    52\n",
       "23    50\n",
       "38    50\n",
       "45    49\n",
       "40    48\n",
       "41    46\n",
       "30    46\n",
       "19    45\n",
       "5     45\n",
       "22    42\n",
       "15    41\n",
       "8     39\n",
       "35    39\n",
       "6     39\n",
       "42    39\n",
       "43    39\n",
       "27    38\n",
       "47    38\n",
       "21    38\n",
       "16    37\n",
       "7     35\n",
       "12    35\n",
       "20    35\n",
       "48    35\n",
       "4     34\n",
       "49    34\n",
       "46    34\n",
       "29    33\n",
       "39    32\n",
       "17    31\n",
       "31    31\n",
       "11    30\n",
       "0     29\n",
       "34    29\n",
       "14    29\n",
       "10    29\n",
       "28    27\n",
       "13    27\n",
       "18    27\n",
       "9     25\n",
       "36    20\n",
       "1     20\n",
       "25    20\n",
       "37    19\n",
       "32    18\n",
       "33    15\n",
       "Name: max, dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#female assigments\n",
    "\n",
    "female['max'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
